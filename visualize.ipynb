{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-clip in /Users/fangzheng/anaconda3/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: ftfy in /Users/fangzheng/anaconda3/lib/python3.10/site-packages (from openai-clip) (6.2.0)\n",
      "Requirement already satisfied: regex in /Users/fangzheng/anaconda3/lib/python3.10/site-packages (from openai-clip) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/fangzheng/anaconda3/lib/python3.10/site-packages (from openai-clip) (4.65.0)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /Users/fangzheng/anaconda3/lib/python3.10/site-packages (from ftfy->openai-clip) (0.2.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/q7mmpvpx5n36lk_j6p9gqk4c0000gn/T/ipykernel_88318/4148449913.py:9: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import torch.optim as optim\n",
    "from tqdm.autonotebook import tqdm\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "# import netron\n",
    "import torch.onnx\n",
    "from torch.autograd import Variable\n",
    "\n",
    "_tokenizer = _Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "device = torch.device('cpu')\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "device\n",
    "# model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self,  classnames, clip_model):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        n_ctx = 4\n",
    "        ctx_init = 'a photo_of a'\n",
    "        dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        vis_dim = clip_model.visual.output_dim\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "        # cfg_imsize = cfg.INPUT.SIZE[0]\n",
    "        # assert cfg_imsize == clip_imsize, f\"cfg_imsize ({cfg_imsize}) must equal to clip_imsize ({clip_imsize})\"\n",
    "\n",
    "        if ctx_init:\n",
    "            # use given words to initialize context vectors\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :].to(device)\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            # random initialization\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx = nn.Parameter(ctx_vectors).to(device)\n",
    "\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
    "        ])).to(device)\n",
    "        \n",
    "        # if cfg.TRAINER.COCOOP.PREC == \"fp16\":\n",
    "        # self.meta_net.half()\n",
    "\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device)  # (n_cls, n_tkn)\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).to(device).type(dtype)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts  # torch.Tensor\n",
    "        self.name_lens = name_lens\n",
    "    \n",
    "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
    "        # dim0 is either batch_size (during training) or n_cls (during testing)\n",
    "        # ctx: context tokens, with shape of (dim0, n_ctx, ctx_dim)\n",
    "        # prefix: the sos token, with shape of (n_cls, 1, ctx_dim)\n",
    "        # suffix: remaining tokens, with shape of (n_cls, *, ctx_dim)\n",
    "\n",
    "        if label is not None:\n",
    "            prefix = prefix[label]\n",
    "            suffix = suffix[label]\n",
    "\n",
    "        prompts = torch.cat(\n",
    "            [\n",
    "                prefix,  # (dim0, 1, dim)\n",
    "                ctx,     # (dim0, n_ctx, dim)\n",
    "                suffix,  # (dim0, *, dim)\n",
    "            ],\n",
    "            dim=1,\n",
    "        ).to(device)\n",
    "\n",
    "        return prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        prefix = self.token_prefix.to(device)\n",
    "        suffix = self.token_suffix.to(device)\n",
    "\n",
    "        ctx = self.ctx                     # (n_ctx, ctx_dim)\n",
    "        print(f'im_features:{im_features.shape}')\n",
    "        print(f'self.meta_net:{self.meta_net}')\n",
    "        bias = self.meta_net(im_features)  # (batch, ctx_dim)\n",
    "\n",
    "        bias = bias.unsqueeze(1)           # (batch, 1, ctx_dim)\n",
    "        ctx = ctx.unsqueeze(0)             # (1, n_ctx, ctx_dim)\n",
    "\n",
    "        ctx_shifted = ctx + bias           # (batch, n_ctx, ctx_dim)\n",
    "        \n",
    "        prompts = []\n",
    "        for ctx_shifted_i in ctx_shifted:\n",
    "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "            pts_i = self.construct_prompts(ctx_i, prefix, suffix)  # (n_cls, n_tkn, ctx_dim)\n",
    "            prompts.append(pts_i)\n",
    "        prompts = torch.stack(prompts)\n",
    "        \n",
    "        return prompts\n",
    "\n",
    "\n",
    "class CustomCLIP(nn.Module):\n",
    "    def __init__(self, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = PromptLearner( classnames, clip_model).to(device)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts.to(device)\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model).to(device)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, image, label=None):\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        image_features = self.image_encoder(image.type(self.dtype)).type(self.dtype).to(device)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        print(f'image:{image.shape}')\n",
    "        print(f'image_features:{image_features.shape}')\n",
    "        prompts = self.prompt_learner(image_features)# 这里有不同，之前的prompt直接就是一个prompt all 的vec了\n",
    "        # (batch_size ,n_cls, n_tkn, ctx_dim)\n",
    "        # print(0.1)\n",
    "        logits = []\n",
    "        for pts_i, imf_i in zip(prompts, image_features):\n",
    "            text_features = self.text_encoder(pts_i.type(self.dtype), tokenized_prompts.type(self.dtype)).type(self.dtype).to(device)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            l_i = logit_scale * imf_i @ text_features.t()\n",
    "            logits.append(l_i)\n",
    "        logits = torch.stack(logits)\n",
    "        print(f'self.prompt_learner.training:{self.prompt_learner.training}')\n",
    "        print(f'logits:{logits}')\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial context: \"a photo of a\"\n",
      "Number of context words (tokens): 4\n"
     ]
    }
   ],
   "source": [
    "customCLIP = CustomCLIP(['1','2'], model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im_features:torch.Size([16, 512])\n",
      "self.meta_net:Sequential(\n",
      "  (linear1): Linear(in_features=512, out_features=32, bias=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (linear2): Linear(in_features=32, out_features=512, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'prompt_learner.png'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "batch_size, dim = 16, 512\n",
    "image_input = torch.randn(batch_size, dim).to(device)\n",
    "output = customCLIP.prompt_learner(image_input)\n",
    "# Get parameters with requires_grad=True\n",
    "params = {name: param for name, param in customCLIP.prompt_learner.named_parameters() if param.requires_grad}\n",
    "\n",
    "# Visualize the model\n",
    "make_dot(output, params=params).render(\"prompt_learner\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text_encoder.png'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts_i = torch.randn([10, 77, 512])\n",
    "imf_i = torch.randn([512])\n",
    "output = customCLIP.text_encoder(pts_i,imf_i)\n",
    "params = {name: param for name, param in customCLIP.text_encoder.named_parameters() if param.requires_grad}\n",
    "make_dot(output, params=params).render(\"text_encoder\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
